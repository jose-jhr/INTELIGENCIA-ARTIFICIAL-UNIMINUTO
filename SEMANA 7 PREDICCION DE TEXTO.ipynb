{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX8gZlVyCCbz"
      },
      "source": [
        "# Modelos del lenguaje con RNNs\n",
        "\n",
        "En esta parte, vamos a entrenar un modelo del lenguaje basado en caracteres con Recurrent Neural Networks. Asimismo, utilizará el modelo para generar texto. En particular,su modelo tiene obras de la literatura clásica en castellano para obtener una red neuronal que sea capaz de \"escribir\" fragmentos literarios.\n",
        "\n",
        "Los entrenamientos para obtener un modelo de calidad podrían tomar cierto tiempo (5-10 minutos por epoch), tener presente que GPU no funciona tan bien con LSTM, por tanto el modelo se debe generar con tiempo.\n",
        "\n",
        "El dataset a utilizar contiene un archivo de texto con el contenido del castellano antiguo de **El Ingenioso Hidalgo Don Quijote de la Mancha**, disponible de manera libre en la página de [Project Gutenberg](https://www.gutenberg.org). Asimismo, usted podrá utilizar otras clases de texto\n",
        "\n",
        "[El ingenioso hidalgo Don Quijote de la Mancha (Miguel de Cervantes)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io)\n",
        "\n",
        "[Compilación de obras teatrales (Calderón de la Barca)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219433&authkey=AKvGD6DC3IRBqmc)\n",
        "\n",
        "[Trafalgar (Benito Pérez Galdós)](https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219434&authkey=AErPCAtMKOI5tYQ)\n",
        "\n",
        "verifique los datos antes de comenzar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI274F8LQC59"
      },
      "source": [
        "## 1. Carga y procesado del texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZNnzvXuqVVm"
      },
      "source": [
        "Primero, descargue el libro e inspeccionar los datos. El fichero a descargar es una versión en .txt del libro de Don Quijote, a la cual se le han borrado introducciones, licencias y otras secciones para dejarlo con el contenido real de la novela."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7tKOZ9BFfki"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "import sys\n",
        "import random\n",
        "import io\n",
        "\n",
        "path = keras.utils.get_file(\n",
        "    fname=\"don_quijote.txt\",\n",
        "    origin=\"https://onedrive.live.com/download?cid=C506CF0A4F373B0F&resid=C506CF0A4F373B0F%219424&authkey=AH0gb-qSo5Xd7Io\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYGLvjLXrUUd"
      },
      "source": [
        "Cundo lea el documento trate de colocar en minuscula el texto para que sea más facil de elaborar.\n",
        "\n",
        "**1.1.** Leer todo el contenido del texto en una única variable ***text*** y convertir el string a minúsculas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WB6FejrrTu9"
      },
      "outputs": [],
      "source": [
        "with open(path, encoding=\"utf8\") as f:\n",
        "  text = f.read().lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knyzgX0XnGfm",
        "outputId": "30a79932-4225-4452-a504-816d10133e74"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2071198"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkgGl8GWtUk8"
      },
      "source": [
        "Puede comprobar que se ha realizado la variación y que el texto se encuentra en minuscula."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMFhe3COFwSD",
        "outputId": "2c21655c-91d5-4330-bd67-343003159402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Longitud del texto: 2071198\n",
            "capítulo primero. que trata de la condición y ejercicio del famoso hidalgo\n",
            "don quijote de la mancha\n",
            "\n",
            "\n",
            "en un lugar de la mancha, de cuyo nombre no quiero acordarme, no ha mucho\n",
            "tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua,\n",
            "rocín flaco y galgo corredor. una olla de algo más vaca que carnero,\n",
            "salpicón las más noches, duelos y quebrantos los sábados, lantejas los\n",
            "viernes, algún palomino de añadidura los domingos, consumían las tres\n",
            "partes de su hacienda. el resto della co\n"
          ]
        }
      ],
      "source": [
        "print(\"Longitud del texto: {}\".format(len(text)))\n",
        "print(text[0:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ7TUXWiyvOj"
      },
      "source": [
        "## 2. Procesado de los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66_Vi_Gyxns"
      },
      "source": [
        "Una de las grandes ventajas de trabajar con modelos que utilizan caracteres en vez de palabras es que no necesita tokenizar el texto (partirlo palabra a palabra). Su modelo funcionará directamente con los caracteres en el texto, incluyendo espacios, saltos de línea, etc.\n",
        "\n",
        "Antes de hacer el ejercicio, debe procesar el texto en entradas y salidas compatibles el modelo. Como sabe, un modelo del lenguaje con RNNs acepta una serie de caracteres y predice el siguiente carácter en la secuencia.\n",
        "\n",
        "* \"*El ingenioso don Qui*\" -> predicción: **j**\n",
        "* \"*El ingenioso don Quij*\" -> predicción: **o**\n",
        "\n",
        "De modo que la entrada y la salida de su modelo necesita ser algo parecido a este esquema. En este punto, podría usar dos formas de preparar los datos para este modelo.\n",
        "\n",
        "1. **Secuencia a secuencia**. La entrada del modelo sería una secuencia y la salida sería esa secuencia trasladada un caracter a la derecha, de modo que en cada instante de tiempo la RNN tiene que predecir el carácter siguiente. Por ejemplo:\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot\n",
        ">* *Output*: l ingenioso don Quijote\n",
        "\n",
        "2. **Secuencia a carácter**. En este variante, pasa una secuencia de caracteres por la RNN y, al llegar al final de la secuencia, se predice el siguiente carácter.\n",
        "\n",
        ">* *Input*:   El ingenioso don Quijot\n",
        ">* *Output*: e\n",
        "\n",
        "En este trabajo, por simplicidad, se utilizará la segunda variante.\n",
        "Utilice secuencias de tamaño *SEQ_LENGTH* caracteres (y elija el hiperparámetro ).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkfJUIxW5m5C"
      },
      "source": [
        "#### 2.1. Obtención de los caracteres y mapas de caracteres\n",
        "\n",
        "Antes que nada, necesita saber qué caracteres aparecen en el texto, ya que tiene que diferenciarlos mediante un índice de 0 a *num_chars* - 1 en el modelo. Obtener:\n",
        "\n",
        "\n",
        "1.   Número de caracteres únicos que aparecen en el texto.\n",
        "2.   Diccionario que asocia char a índice único entre 0 y *num_chars* - 1. Por ejemplo, {'a': 0, 'b': 1, ...}\n",
        "3.   Diccionario reverso de índices a caracteres: {0: 'a', 1: 'b', ...}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bJ0NsbCbupF"
      },
      "outputs": [],
      "source": [
        "chars=sorted(list(set(text)))\n",
        "char_indices = dict((c,i) for i, c in enumerate(chars))\n",
        "indice_char = dict((i, c) for i, c in enumerate(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTMi-94I7HKi",
        "outputId": "d4b0aa45-4c30-48ce-d9db-055271ec1a9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " \"'\": 4,\n",
              " '(': 5,\n",
              " ')': 6,\n",
              " ',': 7,\n",
              " '-': 8,\n",
              " '.': 9,\n",
              " '0': 10,\n",
              " '1': 11,\n",
              " '2': 12,\n",
              " '3': 13,\n",
              " '4': 14,\n",
              " '5': 15,\n",
              " '6': 16,\n",
              " '7': 17,\n",
              " ':': 18,\n",
              " ';': 19,\n",
              " '?': 20,\n",
              " ']': 21,\n",
              " 'a': 22,\n",
              " 'b': 23,\n",
              " 'c': 24,\n",
              " 'd': 25,\n",
              " 'e': 26,\n",
              " 'f': 27,\n",
              " 'g': 28,\n",
              " 'h': 29,\n",
              " 'i': 30,\n",
              " 'j': 31,\n",
              " 'l': 32,\n",
              " 'm': 33,\n",
              " 'n': 34,\n",
              " 'o': 35,\n",
              " 'p': 36,\n",
              " 'q': 37,\n",
              " 'r': 38,\n",
              " 's': 39,\n",
              " 't': 40,\n",
              " 'u': 41,\n",
              " 'v': 42,\n",
              " 'w': 43,\n",
              " 'x': 44,\n",
              " 'y': 45,\n",
              " 'z': 46,\n",
              " '¡': 47,\n",
              " '«': 48,\n",
              " '»': 49,\n",
              " '¿': 50,\n",
              " 'à': 51,\n",
              " 'á': 52,\n",
              " 'é': 53,\n",
              " 'í': 54,\n",
              " 'ï': 55,\n",
              " 'ñ': 56,\n",
              " 'ó': 57,\n",
              " 'ù': 58,\n",
              " 'ú': 59,\n",
              " 'ü': 60}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "#indice_char\n",
        "char_indices\n",
        "#len(chars) # se obtienen 61 caracteres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_B4AWo0ElwA"
      },
      "source": [
        "#### 2.2. Obtención de secuencias de entrada y carácter a predecir\n",
        "\n",
        "Ahora, obtenga la secuencias de entrada en formato texto y los correspondientes caracteres a predecir. Para ello, recorra el texto completo leído anteriormente, obteniendo una secuencia de SEQ_LENGTH caracteres y el siguiente caracter a predecir. Una vez hecho, mueva un carácter a la izquierda y hacer lo mismo para obtener una nueva secuencia y predicción. Guarde las secuencias en una variable ***sequences*** y los caracteres a predecir en una variable ***next_chars***.\n",
        "\n",
        "Por ejemplo, si el texto fuera \"Don Quijote\" y SEQ_LENGTH fuese 5, tendríamos\n",
        "\n",
        "* *sequences* = [\"Don Q\", \"on Qu\", \"n Qui\", \" Quij\", \"Quijo\", \"uijot\"]\n",
        "* *next_chars* = ['u', 'i', 'j', 'o', 't', 'e']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NslxhnnDK6uA"
      },
      "outputs": [],
      "source": [
        "# Definia el tamaño de las secuencias. Puede dejar este valor por defecto.\n",
        "SEQ_LENGTH = 35\n",
        "step=3\n",
        "rawX = []\n",
        "rawy = []\n",
        "\n",
        "for i in range(0, len(text) - SEQ_LENGTH, step):\n",
        "    rawX.append(text[i: i+SEQ_LENGTH])\n",
        "    rawy.append(text[i+SEQ_LENGTH])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Y3AmjYtHdLJ"
      },
      "source": [
        "Indicar el tamaño del training set que acabamos de generar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVWqKxFcbwTu"
      },
      "outputs": [],
      "source": [
        "## SU CÓDIGO AQUÍ\n",
        "#n_sentences=len(rawX)\n",
        "#n_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goGQkKcwpLRJ"
      },
      "source": [
        "Como el Quijote es muy largo y tiene muchas secuencias, puede encontrar problemas de memoria. Por ello, elegija un número máximo de ellas. Si estás corriendo esto localmente y tienes problemas de memoria, puedes reducir el tamaño aún más, pero tenga cuidado porque, a menos datos, peor calidad del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97SXdkJX7HKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee9c4cd-ecd2-4239-c3b7-7b10e1fe5d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "200000\n"
          ]
        }
      ],
      "source": [
        "MAX_SEQUENCES = 200000\n",
        "\n",
        "perm = np.random.permutation(len(rawX)) #Permutar aleatoriamente una secuencia, o devolver un rango permutado.\n",
        "rawX, rawy = np.array(rawX), np.array(rawy)\n",
        "rawX, rawy = rawX[perm], rawy[perm]\n",
        "rawX, rawy = list(rawX[:MAX_SEQUENCES]), list(rawy[:MAX_SEQUENCES])\n",
        "\n",
        "print(len(rawX))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FzgtAbPIs6f"
      },
      "source": [
        "#### 2.3. Obtención de input X y output y para el modelo\n",
        "\n",
        "Finalmente, a partir de los datos de entrenamiento que hemos generado vamos a cree los arrays de datos X e y que pasará a su modelo.\n",
        "\n",
        "Para ello, utilice *one-hot encoding* para el caracteres. Por ejemplo, si sólo tiene 4 caracteres (a, b, c, d), las representaciones serían: (1, 0, 0, 0), (0, 1, 0, 0), (0, 0, 1, 0) y (0, 0, 0, 1).\n",
        "\n",
        "De este modo, **X** tendrá shape *(num_sequences, seq_length, num_chars)* e **y** tendrá shape *(num_sequences, num_chars)*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBwJi3_Dsb2x"
      },
      "outputs": [],
      "source": [
        "X = np.zeros((len(rawX), SEQ_LENGTH , len(chars)))\n",
        "y = np.zeros((len(rawX), len(chars)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F7lUsqLs5T9"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(rawX):\n",
        "    for t, char in enumerate(sentence):\n",
        "        X[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[rawy[i]]] = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxeUxz3HPm3l"
      },
      "source": [
        "## 3. Definición del modelo y entrenamiento\n",
        "\n",
        "Una vez tiene todo,  defina el modelo. Defina un modelo que utilice una **LSTM** con **128 unidades internas**. Si bien el modelo puede definirse de una manera más compleja, para empezar debería bastar con una LSTM más una capa Dense con el *softmax* que predice el siguiente caracter a producir. Adam puede ser una buena elección de optimizador.\n",
        "\n",
        "Una vez el modelo esté definido, entrénelo un poco para asegurarse de que la loss es decreciente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSw2j0btYWZs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16601ac6-bb21-4d23-916a-6de6222bd505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 128)               97280     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 61)                7869      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 105149 (410.74 KB)\n",
            "Trainable params: 105149 (410.74 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model= Sequential()\n",
        "model.add(LSTM(256, input_shape=(SEQ_LENGTH, len(chars))))\n",
        "model.add(Dense(len(chars), activation= \"softmax\"))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-PmivbtXaVhF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYrEhi3mxQoY"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B9RW3-Qxcf9"
      },
      "outputs": [],
      "source": [
        "#model.fit(X, y, batch_size=128, epochs=20, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUFHS4kHkyY"
      },
      "source": [
        "Para ver cómo evoluciona SU modelo del lenguaje,  genere texto según va entrenando. Para ello, programe una función que, utilizando el modelo en su estado actual, genere texto, con la idea de ver cómo se va generando texto al entrenar cada epoch.\n",
        "\n",
        "En el código de abajo puede ver una función auxiliar para obtener valores de una distribución multinomial. Esta función se usará para muestrear el siguiente carácter a utilizar según las probabilidades de la salida de softmax (en vez de tomar directamente el valor con la máxima probabilidad, obtiene un valor aleatorio según la distribución de probabilidad dada por softmax, de modo que los resultados serán más diversos, pero seguirán teniendo \"sentido\" ya que el modelo tenderá a seleccionar valores con más probabilidad).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoGYpWOHd7Lr"
      },
      "outputs": [],
      "source": [
        "def sample(probs, temperature=1.0):\n",
        "    \"\"\"Nos da el índice del elemento a elegir según la distribución\n",
        "    de probabilidad dada por probs.\n",
        "\n",
        "    Args:\n",
        "      probs es la salida dada por una capa softmax:\n",
        "        probs = model.predict(x_to_predict)[0]\n",
        "\n",
        "      temperature es un parámetro que nos permite obtener mayor\n",
        "        \"diversidad\" a la hora de obtener resultados.\n",
        "\n",
        "        temperature = 1 nos da la distribución normal de softmax\n",
        "        0 < temperature < 1 hace que el sampling sea más conservador,\n",
        "          de modo que sampleamos cosas de las que estamos más seguros\n",
        "        temperature > 1 hace que los samplings sean más atrevidos,\n",
        "          eligiendo en más ocasiones clases con baja probabilidad.\n",
        "          Con esto, tenemos mayor diversidad pero se cometen más\n",
        "          errores.\n",
        "    \"\"\"\n",
        "    # Cast a float64 por motivos numéricos\n",
        "    probs = np.asarray(probs).astype('float64')\n",
        "\n",
        "    # logaritmo de probabilidades y aplicamos reducción\n",
        "    # por temperatura.\n",
        "    probs = np.log(probs) / temperature\n",
        "\n",
        "    # Volvemos a aplicar exponencial y normalizamos de nuevo\n",
        "    exp_probs = np.exp(probs)\n",
        "    probs = exp_probs / np.sum(exp_probs)\n",
        "\n",
        "    # Hacemos el sampling dadas las nuevas probabilidades\n",
        "    # de salida (ver doc. de np.random.multinomial)\n",
        "    samples = np.random.multinomial(1, probs, 1)\n",
        "    return np.argmax(samples)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fejfZldd4ou"
      },
      "source": [
        "Utilizando la función anterior y el modelo entrenado, añadir un callback a al modelo para que, según vaya entrenando, se vean los valores que resultan de generar textos con distintas temperaturas al acabar cada epoch.\n",
        "\n",
        "Para ello, abajo tienedisponible el callback *on_epoch_end*. Esta función elige una secuencia de texto al azar en el texto disponible en la variable\n",
        "text y genera textos de longitud *GENERATED_TEXT_LENGTH* según las temperaturas en *TEMPERATURES_TO_TRY*, utilizando para ello la función *generate_text*.\n",
        "\n",
        "Complete la función *generate_text* de modo que utilice el modelo y la función sample para generar texto.\n",
        "\n",
        "NOTA: Cuando haga model.predict, es aconsejable usar verbose=0 como argumento para evitar que la función imprima valores de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOEZvnBXkODd"
      },
      "outputs": [],
      "source": [
        "TEMPERATURES_TO_TRY = [0.2] #, 0.5, 1.0, 1.2]\n",
        "GENERATED_TEXT_LENGTH = 300\n",
        "\n",
        "def generate_text(seed_text, model, length=300, temperature=1, max_length=30):\n",
        "    \"\"\"Genera una secuencia de texto a partir de seed_text utilizando model.\n",
        "\n",
        "    La secuencia tiene longitud length y el sampling se hace con la temperature\n",
        "    definida.\n",
        "    \"\"\"\n",
        "\n",
        "    # Aquí guardaremos nuestro texto generado, que incluirá el\n",
        "    # texto origen\n",
        "    generated = seed_text\n",
        "\n",
        "    # Utilizar el modelo en un bucle de manera que generemos\n",
        "    # carácter a carácter. Habrá que construir los valores de\n",
        "    # X_pred de manera similar a como hemos hecho arriba, salvo que\n",
        "    # aquí sólo se necesita una oración\n",
        "    # Nótese que el x que utilicemos tiene que irse actualizando con\n",
        "    # los caracteres que se van generando. La secuencia de entrada al\n",
        "    # modelo tiene que ser una secuencia de tamaño SEQ_LENGTH que\n",
        "    # incluya el último caracter predicho.\n",
        "\n",
        "    ### TU CÓDIGO AQUÍ\n",
        "    prediction = []\n",
        "\n",
        "        #textReturn\n",
        "    textReturn = \"\"\n",
        "\n",
        "    for i in range(length):\n",
        "        # Make numpy array to hold seed\n",
        "        X = np.zeros((1, len(generated), len(chars) ))\n",
        "\n",
        "        # Set one-hot vectors for seed sequence\n",
        "        for t, char in enumerate(seed_text):\n",
        "            X[0, t, char_indices[char]] = 1\n",
        "\n",
        "        # Generate prediction for next character\n",
        "        preds = model.predict(X, verbose=0)[0]\n",
        "        # Choose a character from the prediction probabilities\n",
        "        next_index = sample(preds,0.2)\n",
        "        next_char = indice_char[next_index]\n",
        "\n",
        "        prediction.append(next_char)\n",
        "        textReturn = textReturn+next_char\n",
        "        # Add the predicted character to the seed sequence so the next prediction\n",
        "        # includes this character in it's seed.\n",
        "        #generated += next_char\n",
        "        seed_text = seed_text[1:] + next_char\n",
        "\n",
        "        print(next_char, end= \" \");\n",
        "        # Flush so we can see the prediction as it's generated\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    prediction = ''.join(prediction)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    ### FIN DE TU CÓDIGO\n",
        "    return textReturn\n",
        "\n",
        "\n",
        "def on_epoch_end(epoch, logs):\n",
        "  print(\"\\n\\n\\n\")\n",
        "\n",
        "  # Primero, seleccionamos una secuencia al azar para empezar a predecir\n",
        "  # a partir de ella\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  for temperature in TEMPERATURES_TO_TRY:\n",
        "    print(\"------> Epoch: {} - Generando texto con temperature {}\".format(\n",
        "        epoch + 1, temperature))\n",
        "\n",
        "    generated_text = generate_text(seed_text, model,\n",
        "                                   GENERATED_TEXT_LENGTH, temperature)\n",
        "    print(\"Seed: {}\".format(seed_text))\n",
        "    print(\"Texto generado: {}\".format(generated_text))\n",
        "\n",
        "\n",
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSMYZ2JdrSJg"
      },
      "source": [
        "Entrene ahora su modelo. No se olvides de añadir *generation_callback* a la lista de callbacks utilizados en fit(). Ya que las métricas de clasificación no son tan críticas aquí (no nos importa tanto acertar el carácter exacto, sino obtener una distribución de probabilidad adecuada), no es necesario monitorizar la accuracy ni usar validation data, si bien puedes añadirlos para asegurarte de que todo está en orden.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rawX[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivMQDVm1ej9o",
        "outputId": "858b26ad-ce38-46d8-e1b9-4732e7325f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uve hasta el último trance de la vi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def on_epoch_end(epoch, logs):\n",
        "  # Print training loss and accuracy\n",
        "  print(\"\\nEpoch: {}, Loss: {:.4f}, Accuracy: {:.2f}%\".format(\n",
        "      epoch + 1, logs['loss'], logs['accuracy'] * 100))\n",
        "  start_pos = random.randint(0, len(text) - SEQ_LENGTH - 1)\n",
        "  seed_text = text[start_pos:start_pos + SEQ_LENGTH]\n",
        "  # Generate text with a specific temperature\n",
        "  temperature = 0.2\n",
        "  #seed_text = rawX[0]\n",
        "  generated_text = generate_text(seed_text, model, length=300, temperature=temperature)\n",
        "  print(\"\\n Text Send Text: \\n\", seed_text)\n",
        "  print(\"\\n Generated Text: \\n\", generated_text)"
      ],
      "metadata": {
        "id": "YdlYJq3td5Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n"
      ],
      "metadata": {
        "id": "hU-EX6hyd9jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oT7pNvjrP2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "4b529e69-12e3-4dcd-de39-1c9d65c5f27c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 1, Loss: 2.5060, Accuracy: 27.67%\n",
            "e   m e n t i e n t e   d e   l a   l a   p o n t a   d e   d e   d e   l a   c u e n t e   d e   d e   c o n t o   d e   d e   d e   l a   d i e n t o   d e   c o r a   d e   l a   c o n t a   d e   l a   l o   c e r o s   d e   l a   l a   l a   d e   l o   d e   d e   d e   d e   d e   t e n t a   d e   l a   l a   l a   l a   p o r a   d e   c o n t e   d e   l a   d e   d e   l a   l a   d i e n t a   d e   h a   s u n t o   d e   d e   d e   d e   d e   t e n t a   d e   l a   d e   d e   t e   d e   c e n t a   d e   l o   c o n t a   d e   p o n   d e   l a   m e n t e s   l a   s u n \n",
            " Text Send Text: \n",
            " ncuentros firme bien merece llamars\n",
            "\n",
            " Generated Text: \n",
            " e mentiente de la la ponta de de de la cuente de de conto de de de la diento de cora de la conta de la lo ceros de la la la de lo de de de de de tenta de la la la la pora de conte de la de de la la dienta de ha sunto de de de de de tenta de la de de te de centa de lo conta de pon de la mentes la sun\n",
            "\n",
            "Epoch: 2, Loss: 2.1289, Accuracy: 34.84%\n",
            "o r   e l   s u n   p a r   d e   l a   p a r a   d e   s e   l a   d e   l a   c o n   e l   s u n t e r   d e   s e   c o r   e l   s u n   e s t o   d e   l o   q u e   d e   l a   c o n   e n   a l l a   d e   l a   l a   m a r t e   d e   l a   p a r a   l e   c o n   a l   c o r   e l   s u n   l a   p o r   e n   q u e   l o   s e   l a   p o n d e   d e   l o   l a   l a   l a   v i e n t o   d e   l a   l a   c a r e   t o n   a l   p a r   e n   e l   c i n o   d e   l a   c a r a   d e   l o   q u e   l a   c o n   l o   q u e   l o   l a   c o n   a l l a   d e   l a   c a r   e n \n",
            " Text Send Text: \n",
            " nta micomicona, y\n",
            "yo fuera conde, p\n",
            "\n",
            " Generated Text: \n",
            " or el sun par de la para de se la de la con el sunter de se cor el sun esto de lo que de la con en alla de la la marte de la para le con al cor el sun la por en que lo se la ponde de lo la la la viento de la la care ton al par en el cino de la cara de lo que la con lo que lo la con alla de la car en\n",
            "\n",
            "Epoch: 3, Loss: 2.0287, Accuracy: 37.70%\n",
            "y   a   l a   d e   c o n t a d o   e s t a n d e   d e   l a   d e   s u n c i r   e n   e l   c u i n t o   d e   p a r a   d e   d e   l a   c o n d o ,   s e   l a   c u e n t a   d e   l a   c u e n t a   d e   l a   m a n d a r   e n   e s t a   d e   d e   s u   d e   e n   e l   c u e n t e   d e   l a   c u a n t a   d e   l a   c a n t e r a   d e   s u   c u r a   d e   l a   m a n d a   s e   l a   d e   d e   s u   d e   l a   c o n d o   d e   p o r   d e   c u e s t o   d e   l a   c a d a d o   d e   l a   m a n t a   d e   d e   s e   p a r a   d e   l a   c u e n t a   d e   \n",
            " Text Send Text: \n",
            "  estaba sancho bizmado y acostado, \n",
            "\n",
            " Generated Text: \n",
            " y a la de contado estande de la de suncir en el cuinto de para de de la condo, se la cuenta de la cuenta de la mandar en esta de de su de en el cuente de la cuanta de la cantera de su cura de la manda se la de de su de la condo de por de cuesto de la cadado de la manta de de se para de la cuenta de \n",
            "\n",
            "Epoch: 4, Loss: 1.9636, Accuracy: 39.54%\n",
            "  q u e   l a   m a n t e   d e   l a   m u s t o   d e   l a   d e   l a   c a n t e   d e   l a   m a l a r   a   l a   m e r a s   d e   l a s   a l l a   d e   s u   l a s   a l l a r   d e   l a   c a n t a n   q u e   l a   c a n t a   d e   m i n o   d e   l a   m i n t o ,   y   a   l a   m i n t o   a   l a   m a n t a r   a   l o s   a l l a r   a   l a   m i s t o   d e   l a   m i s t o   a l   c a n t e   l a   m a n t e n d o   d e   l a   c a n t e   l a   m i n t o   s e   l a   s u   l a   p a r e s   q u e   l a   m e r o s   a   l a   m i n t o   d e   l a   c o n   l a   m \n",
            " Text Send Text: \n",
            " antasía de todo aquello que leía en\n",
            "\n",
            " Generated Text: \n",
            "  que la mante de la musto de la de la cante de la malar a la meras de las alla de su las allar de la cantan que la canta de mino de la minto, y a la minto a la mantar a los allar a la misto de la misto al cante la mantendo de la cante la minto se la su la pares que la meros a la minto de la con la m\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-1c6010d840cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgeneration_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "history = model.fit(X, y, batch_size=128, epochs=40, verbose=0, callbacks=[generation_callback],validation_split=0.2)\n",
        "history.history['accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model57%.h5')"
      ],
      "metadata": {
        "id": "ytbWyz8Bk48H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val = generate_text(rawX[0], model)\n",
        "print(val)"
      ],
      "metadata": {
        "id": "uTcDplx0k8pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wtHy-zt4lNbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rawX[0])"
      ],
      "metadata": {
        "id": "pH8KVFYTlRDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBbmz9DMhVhc"
      },
      "source": [
        "## Entregable\n",
        "\n",
        "Complete los apartados anteriores para entrenar modelos del lenguaje que sean capaces de generar texto con cierto sentido. Comentar los resultados obtenidos y cómo el modelo va mejorando época a época. Comentar las diferencias apreciadas al utilizar diferentes valores de temperatura. Entregar al menos la salida de un entrenamiento completo con los textos generados época a época.\n",
        "\n",
        "El objetivo no es conseguir generar pasajes literarios con coherencia, sino obtener lenguaje que se asemeje en cierta manera a lo visto en el texto original y donde las palabras sean reconocibles como construcciones en castellano. Como ejemplo de lo que se puede conseguir, este es el resultado de generar texto después de 10 epochs y con temperature 0.2:\n",
        "\n",
        "\n",
        "```\n",
        "-----> Epoch: 10 - Generando texto con temperature 0.2\n",
        "Seed: o le cautivaron y rindieron el\n",
        "Texto generado: o le cautivaron y rindieron el caballero de la caballería de la mano de la caballería del cual se le dijo:\n",
        "\n",
        "-¿quién es el verdad de la caballería de la caballería de la caballería de la caballería de la caballería, y me ha de habían de la mano que el caballero de la mano de la caballería. y que no se le habían de la mano de la c\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJp9Ds55s2O-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GneURbWZxN8d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}